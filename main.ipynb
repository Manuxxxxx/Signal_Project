{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgTransformations\n",
    "import utils_jupyter\n",
    "import utils\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image enhancement in low-light conditions with MEF techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When capturing images with a single exposure level, it is likely that a portion of detail will be lost; this is caused by the potential presence of shadows or even highligths. As a consequence, capturing clear images in these conditions results challenging.\n",
    "\n",
    "Multi-exposure fusion (MEF) methods are techniques used in digital imaging to combine multiple images of the same scene taken with different exposure settings. The goal is to create a single image that captures a wider range of luminance than any of the individual source images. This is particularly useful in high dynamic range (HDR) imaging, where the goal is to reproduce the full range of brightness levels found in real-world scenes. The image resulting from this procedure preserves details from both dark and bright areas, enhancing overall quality.\n",
    "\n",
    "In order to produce an image with enhanced quality, multiple capturings of the same scene (at different light conditions) have to be provided to these methods. The following widget allow for the selection of a set of images, taken from the folder `MEFDatabase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize path for images\n",
    "dirname = os.path.abspath('')\n",
    "folder = os.path.join(dirname, 'MEFDatabase/source image sequences/')\n",
    "images = [] \n",
    "\n",
    "# Pass the images list to the interactive selector\n",
    "utils_jupyter.interactive_image_selector(folder, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average fusion\n",
    "\n",
    "The first method presented implements the image enhancement process as a simple pixel-wise average of the multi-exposed images. While being computationally effective, it is not designed to preserve details as much as other more complex methods do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_fusion(images):\n",
    "    \"\"\"Fuse images by taking the simple average.\"\"\"\n",
    "    utils_jupyter.imageCheck(images)\n",
    "    \n",
    "    images_float = [img.astype(np.float32) for img in images]\n",
    "    fused = np.mean(images_float, axis=0)\n",
    "    fused = np.clip(fused, 0, 255).astype(np.uint8)\n",
    "    return ('Simple Average', fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian pyramid fusion\n",
    "\n",
    "A Lapliacian Pyramid is a linear invertible image representation consisting of a set of band-pass images spaced an octave apart, plus a low-frequency residual. Given the following operations:\n",
    "1. $d(I)$: a downsampling operation that blurs and decimates a $i \\times j$ image $I$ to produce and new $\\frac{i}{2} \\times \\frac{j}{2}$ image;\n",
    "2. $u(I)$: an updampling operation that smooths and extends a $i \\times j$ image $I$ to produce and new $2i \\times 2j$ image;\n",
    "\n",
    "the Lapliacian Pyramid of an image can be build thoug the following process:\n",
    "1. Build the Gaussian Pyramid $G(I)=[I_0, I_1, ..., I_K]$, where $I_k$ corresponds to $k$ repeated applications of $d$ on $I$;\n",
    "2. Compute the coefficients $h_k$ at each level $k$, which capture the image structure at that particular scale, by taking the difference between adjacent levels of the Gaussian Pyramid (in order to have compatible sizes, the image corresponding to the higher level is upsampled):\n",
    "\n",
    "```math \n",
    "h_k=G_k(I)-u(G_k+1(I))\n",
    "```\n",
    "\n",
    "The MEF method based of Laplacian Pyramids technique woks by computing all the Lapliacian Pyramids corresponding to the input images; these pyramids are later fused in a single LP, where each layer is the result of averaging the correspnding levels from the inputs, thus combining the details from all images. The resulting LP is ultimately used to reconstruct the image, progressively upsampling each level up to the original size and summing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_pyramid_fusion(images, levels=4):\n",
    "    \"\"\"Fuse images using Laplacian Pyramid Fusion.\"\"\"\n",
    "    utils_jupyter.imageCheck(images)\n",
    "\n",
    "    # Build Gaussian pyramids for each image\n",
    "    gaussian_pyramids = []\n",
    "    for img in images:\n",
    "        gp = [img.astype(np.float32)]\n",
    "        for i in range(levels):\n",
    "            img = cv2.pyrDown(img)\n",
    "            gp.append(img.astype(np.float32))\n",
    "        gaussian_pyramids.append(gp)\n",
    "\n",
    "    # Build Laplacian pyramids for each image\n",
    "    laplacian_pyramids = []\n",
    "    for gp in gaussian_pyramids:\n",
    "        lp = []\n",
    "        for i in range(len(gp) - 1):\n",
    "            size = (gp[i].shape[1], gp[i].shape[0])\n",
    "            GE = cv2.pyrUp(gp[i+1], dstsize=size)\n",
    "            L = gp[i] - GE\n",
    "            lp.append(L)\n",
    "        lp.append(gp[-1])\n",
    "        laplacian_pyramids.append(lp)\n",
    "\n",
    "    # Fuse Laplacian pyramids by averaging each level\n",
    "    fused_pyramid = []\n",
    "    for level in range(levels + 1):\n",
    "        # Stack corresponding level of each pyramid and take mean\n",
    "        layer_stack = np.array([lp[level] for lp in laplacian_pyramids])\n",
    "        fused_layer = np.mean(layer_stack, axis=0)\n",
    "        fused_pyramid.append(fused_layer)\n",
    "\n",
    "    # Reconstruct the fused image from the fused pyramid\n",
    "    fused = fused_pyramid[-1]\n",
    "    for i in range(levels, 0, -1):\n",
    "        size = (fused_pyramid[i-1].shape[1], fused_pyramid[i-1].shape[0])\n",
    "        fused = cv2.pyrUp(fused, dstsize=size)\n",
    "        fused = fused + fused_pyramid[i-1]\n",
    "    \n",
    "    fused = np.clip(fused, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return (('Laplacian Pyramid - levels:'+str(levels)), fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain transform fusion\n",
    "\n",
    "The `domain_transform_fusion` function is designed to fuse the input images using the Domain Transform filtering technique. First, the input images are normalized to the range $[0,1]$ by converting them to `float32` and dividing by 255, ensuring that all pixel values are between 0 and 1; then, for each of them, three types of weights are computed:\n",
    "1. **Contrast**: highlights edges and areas with high contrast, and is calculated using the absolute Laplacian of the grayscale version of the image;\n",
    "2. **Saturation**: emphasizes areas with high color saturation, and is computed as the standard deviation across the three color channels;\n",
    "3. **Well-exposedness**: measures how well-exposed each pixel is by using a the product of the channel's Gaussian responses centered around 0.5.\n",
    "\n",
    "The weights are combined into a single weight map, and a small epsilon value is added to prevent division by 0, which otherwise could occur in later calculations. The resulting weight map is smoothed using Domain Transform filtering; this step can be done using a built-in function provided by OpenCV (`cv2.ximgproc.dtFilter`), but we also propose a custom implementation of the same procedure (`dt_filter_homebrew`).\n",
    "\n",
    "Ultimately, the smoothed images are normalized, fused together as a weighted sum and the final image is reconverted into an 8-bit unsigned integer image with values in the range $[0,255]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_transform_fusion(images, sigmaSpatial=60, sigmaColor=0.4, epsilon=1e-6, homebrew_dt=False, homebrew_iteration=3):\n",
    "    \"\"\" Fuse images using Domain Transform filtering to refine weight maps. \"\"\"\n",
    "    \n",
    "    # Normalize images to [0, 1]\n",
    "    imgs = [img.astype(np.float32) / 255.0 for img in images]\n",
    "    weight_maps = []\n",
    "    desc = None\n",
    "    \n",
    "    for img in imgs:\n",
    "        # Compute contrast weight: use absolute Laplacian on grayscale image\n",
    "        gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        contrast = np.abs(cv2.Laplacian(gray, cv2.CV_32F))\n",
    "        \n",
    "        # Compute saturation weight: standard deviation across color channels\n",
    "        saturation = np.std(img, axis=2)\n",
    "        \n",
    "        # Compute well-exposedness weight: product of Gaussian responses for each channel\n",
    "        well_exposedness = np.exp(-0.5 * ((img - 0.5) / 0.2) ** 2)\n",
    "        well_exposedness = np.prod(well_exposedness, axis=2)\n",
    "        \n",
    "        # Combine the weights with a small epsilon to avoid zeros\n",
    "        weight = (contrast + epsilon) * (saturation + epsilon) * (well_exposedness + epsilon)\n",
    "        \n",
    "        # Smooth the weight map using Domain Transform filtering.\n",
    "        # The guide image can be the original color image.\n",
    "        # Mode 1 (DTF_RF) applies recursive filtering.\n",
    "        if(homebrew_dt):\n",
    "            desc = \"Simple Domain Transform\"\n",
    "            # guidance = img.astype(np.float32) / 255.0\n",
    "            smooth_weight = dt_filter_homebrew(img, weight, sigmaSpatial, sigmaColor, num_iterations=homebrew_iteration)\n",
    "        else:\n",
    "            desc =\"OpenCV Domain Transform\"\n",
    "            smooth_weight = cv2.ximgproc.dtFilter(img, weight.astype(np.float32), sigmaSpatial, sigmaColor, mode=1)\n",
    "        \n",
    "        weight_maps.append(smooth_weight)\n",
    "    \n",
    "    # Normalize the weight maps so that they sum to 1 at every pixel\n",
    "    weight_sum = np.sum(np.array(weight_maps), axis=0) + epsilon\n",
    "    normalized_weights = [w / weight_sum for w in weight_maps]\n",
    "    \n",
    "    # Fuse the images using the normalized weight maps\n",
    "    fused = np.zeros_like(imgs[0])\n",
    "    for img, w in zip(imgs, normalized_weights):\n",
    "        # Expand weight map to three channels for element-wise multiplication\n",
    "        w3 = np.repeat(w[:, :, np.newaxis], 3, axis=2)\n",
    "        fused += img * w3\n",
    "\n",
    "    # Convert back to 0-255 range and uint8\n",
    "    fused_image = np.clip(fused * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return (\"Domain Transform Fusion\"+\" - \"+desc, fused_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homebrew domain tranform filter\n",
    "\n",
    "A naive Python implementation of the Domain Transform filter for edge-aware smoothing.\n",
    "    \n",
    "This function applies recursive filtering along horizontal and vertical directions\n",
    "guided by the guidance image. It computes domain transform coefficients based on\n",
    "intensity differences (or summed color differences) and then performs forward/backward\n",
    "passes to filter the input src image.\n",
    "\n",
    "It takes as parameters:\n",
    "1. A guidance image as a `float32` array in $[0, 1]$, either single or multi-channel;\n",
    "2. The input image to filter (with the same shape as the guidance);\n",
    "3. `sigmaSpatial`: the spatial extent of the filter;\n",
    "4. `sigmaColor`: the intensity with which color differences affect the filtering\n",
    "5. The number of recursive iterations, with higher numbers approximating a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_filter_homebrew(guidance, src, sigmaSpatial=60, sigmaColor=0.4, num_iterations=3):\n",
    "    H, W = guidance.shape[:2]\n",
    "\n",
    "    def compute_diff_x(g):\n",
    "        diff = np.zeros((H, W), dtype=np.float32)\n",
    "        for i in range(H):\n",
    "            for j in range(1, W):\n",
    "                if g.ndim == 3:\n",
    "                    # Sum differences over channels\n",
    "                    diff[i, j] = np.sum(np.abs(g[i, j] - g[i, j-1]))\n",
    "                else:\n",
    "                    diff[i, j] = abs(g[i, j] - g[i, j-1])\n",
    "        return diff\n",
    "\n",
    "    def compute_diff_y(g):\n",
    "        diff = np.zeros((H, W), dtype=np.float32)\n",
    "        for i in range(1, H):\n",
    "            for j in range(W):\n",
    "                if g.ndim == 3:\n",
    "                    diff[i, j] = np.sum(np.abs(g[i, j] - g[i-1, j]))\n",
    "                else:\n",
    "                    diff[i, j] = abs(g[i, j] - g[i-1, j])\n",
    "        return diff\n",
    "\n",
    "    # --- Horizontal Filtering ---\n",
    "    # Compute horizontal differences and coefficients\n",
    "    dI_dx = compute_diff_x(guidance)\n",
    "    dt_x = 1 + dI_dx / sigmaColor  # Domain transform along x\n",
    "    a_x = np.exp(- (np.sqrt(2) / sigmaSpatial) * dt_x)\n",
    "    \n",
    "    # Initialize result with src (make a copy so as not to alter original data)\n",
    "    result = src.copy()\n",
    "\n",
    "    # Apply recursive filtering horizontally (for each iteration, do a forward and backward pass)\n",
    "    for _ in range(num_iterations):\n",
    "        # Forward pass (left to right)\n",
    "        for i in range(H):\n",
    "            for j in range(1, W):\n",
    "                result[i, j] = a_x[i, j] * result[i, j-1] + (1 - a_x[i, j]) * result[i, j]\n",
    "        # Backward pass (right to left)\n",
    "        for i in range(H):\n",
    "            for j in range(W-2, -1, -1):\n",
    "                result[i, j] = a_x[i, j+1] * result[i, j+1] + (1 - a_x[i, j+1]) * result[i, j]\n",
    "    \n",
    "    # --- Vertical Filtering ---\n",
    "    # Compute vertical differences and coefficients\n",
    "    dI_dy = compute_diff_y(guidance)\n",
    "    dt_y = 1 + dI_dy / sigmaColor  # Domain transform along y\n",
    "    a_y = np.exp(- (np.sqrt(2) / sigmaSpatial) * dt_y)\n",
    "    \n",
    "    # Apply recursive filtering vertically\n",
    "    for _ in range(num_iterations):\n",
    "        # Forward pass (top to bottom)\n",
    "        for j in range(W):\n",
    "            for i in range(1, H):\n",
    "                result[i, j] = a_y[i, j] * result[i-1, j] + (1 - a_y[i, j]) * result[i, j]\n",
    "        # Backward pass (bottom to top)\n",
    "        for j in range(W):\n",
    "            for i in range(H-2, -1, -1):\n",
    "                result[i, j] = a_y[i+1, j] * result[i+1, j] + (1 - a_y[i+1, j]) * result[i, j]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wavelet fusion\n",
    "\n",
    "The wavelet transform decomposes a signal into wavelets coefficient, which represent the signal's content at different scales and positions. Unlike Fourier transform, WT allow capturing both frequency and location informations, by decomposing the signal into shifted and scaled version of a mother wave: this makes it particularly useful for images with varying frequency content.\n",
    "\n",
    "The library PyWavelets (`pywt`) provides a set of tools for wavelet transforms. In particular, the function `wavedec2` performs a 2-dimensional discrete WT, decomposing an image into approximation and detail coefficients:\n",
    "1. Approximation coefficients represent the low-frequency content of the image, capturing its general structure;\n",
    "2. Detail coefficients capture, for each level, the high-frequency content along three orientation, which are horizontal, vertical and diagonal.\n",
    "\n",
    "The MEF technique based on wavelet transforms is performed through the fusion of the coefficient obtained by each input image; in particular, the approximation coefficient is computed by averaging the input ones, while the detail coefficients are fused through the max-absolute rule, which selects the coefficient with the highest value for each level and orientation. Ultimately, the resulting coefficient are used to reconstruct the image using the function `wavedec2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_fusion(images, wavelet='db1', level=2):\n",
    "    \"\"\" Fuse images using wavelet transform-based fusion. \"\"\"\n",
    "    images = [cv2.split(img) for img in images]\n",
    "    channels = list(zip(*images))  # Separate into R, G, B channel lists\n",
    "\n",
    "    fused_channels = []\n",
    "\n",
    "    # Comupte each channel separately\n",
    "    for channel_images in channels:\n",
    "        # Convert images to float32 and normalize to [0,1]\n",
    "        imgs = [img.astype(np.float32) / 255.0 for img in channel_images]\n",
    "\n",
    "        # Compute coefficients of each image\n",
    "        coeffs_list = [pywt.wavedec2(img, wavelet=wavelet, level=level) for img in imgs]\n",
    "\n",
    "        # Fuse coefficients\n",
    "        fused_coeffs = []\n",
    "\n",
    "        # Fuse approximation coefficients using the mean\n",
    "        fused_approx = np.mean([coeffs[0] for coeffs in coeffs_list], axis=0)\n",
    "        fused_coeffs.append(fused_approx)\n",
    "\n",
    "        # Fuse detail coefficients for each level\n",
    "        for lvl in range(1, level + 1):\n",
    "            fused_details = []\n",
    "            for i in range(3):  # Horizontal, vertical, diagonal details\n",
    "                detail_coeffs = np.array([coeffs[lvl][i] for coeffs in coeffs_list])\n",
    "                # Use max-absolute rule: choose coefficient with highest absolute value\n",
    "                fused_detail = np.choose(np.argmax(np.abs(detail_coeffs), axis=0), detail_coeffs)\n",
    "                fused_details.append(fused_detail)\n",
    "            fused_coeffs.append(tuple(fused_details))\n",
    "\n",
    "        # Reconstruct the fused image\n",
    "        fused_img = pywt.waverec2(fused_coeffs, wavelet=wavelet)\n",
    "\n",
    "        # Ensure the output image has the same size as the original\n",
    "        h, w = channel_images[0].shape\n",
    "        fused_img = fused_img[:h, :w]\n",
    "\n",
    "        # Clip and convert back to uint8 in the range [0,255]\n",
    "        fused_img = np.clip(fused_img, 0, 1)\n",
    "        fused_img = (fused_img * 255).astype(np.uint8)\n",
    "\n",
    "        fused_channels.append(fused_img)\n",
    "\n",
    "    # Merge channels back for color images\n",
    "    fused_img = cv2.merge(fused_channels)\n",
    "\n",
    "    return (\"Wavelet Fusion\", fused_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exposure compensation\n",
    "\n",
    "The following function aims at producing an enhanced image by exposure compensation; it does so by leveraging the different exposure levels present in the input images, and is a simplified version of more sophisticated methods adopted in modern cameras.\n",
    "\n",
    "The function first converts each input image to grayscale calculates the average luminance across all of them. For each image, a gain factor is computed as the ratio of the average luminance to the image's luminance, which serves as a reference for adjusting the brightness of each image; this is done by multiplying each image's pixel values with the corresponding gain factor. The adjusted images are stacked together, and the final fused image is obtained by averaging the pixel values across all the adjusted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_compensation_fusion(images):\n",
    "    # Convert images to grayscale for luminance computation\n",
    "    luminances = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
    "    \n",
    "    # Calculate average luminance across all images\n",
    "    avg_luminance = np.mean(luminances, axis=0)\n",
    "    \n",
    "    # Compute gain factors for each image (avoid division by zero)\n",
    "    gain_factors = [avg_luminance / (lum + 1e-6) for lum in luminances]\n",
    "    \n",
    "    # Apply gain factors with NumPy multiplication (after converting to float)\n",
    "    compensated_images = []\n",
    "    for img, gf in zip(images, gain_factors):\n",
    "        # Convert image to float for multiplication\n",
    "        img_float = img.astype(np.float32)\n",
    "        # Expand gain factor dimensions and convert to float32\n",
    "        gf_expanded = gf[:, :, np.newaxis].astype(np.float32)\n",
    "        # Multiply and clip the result\n",
    "        compensated = np.clip(img_float * gf_expanded, 0, 255).astype(np.uint8)\n",
    "        compensated_images.append(compensated)\n",
    "    \n",
    "    # Fuse images by averaging pixel values\n",
    "    fused_image = np.mean(np.stack(compensated_images, axis=0), axis=0).astype(np.uint8)\n",
    "    \n",
    "    return ('Exposure Compensation', fused_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exposure fusion (T. Mertens)\n",
    "\n",
    "The main concepts presented by the previous methods are implmenented in the MEF technique proposed by T. Mertens; in this Exposure Fusion method, the goal is to assign less weight to flat or colorless regions (due to under-exposure or over-exposure), while preserving areas containing bright colors and details.\n",
    "\n",
    "Similarly as in the Domain Transform method, the weights of each pixel is computed as the sum of contrast (The asbolute value of the Laplacian filter response to the grayscale image), the saturation (computed as the standard deviation) and the well-exposedness (based on how close the pixel intensity is close to 0.5), following the formula below:\n",
    "\n",
    "```math \n",
    "W_{i,j,k} = (C_{i,j,k})^{\\omega _C} \\times (S_{i,j,k})^{\\omega _S} \\times (E_{i,j,k})^{\\omega _E},\n",
    "``` \n",
    "\n",
    "where $C$, $S$ and $E$ are, respectively, contrast, saturation and well-exposedness of pixel $(i,j)$ from image $k$, weighted by the corresponding exponents $\\omega _C$, $\\omega _S$ and $\\omega _E$ (for simplicity, these weights are set to 1).\n",
    "\n",
    "These weights are later normalized and, ultimately, the final image is computed as the weight sum of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_fusion(images):\n",
    "    # Convert images to float32 for processing\n",
    "    images = [img.astype(np.float32) / 255.0 for img in images]\n",
    "\n",
    "    # Compute Laplacian contrast weight (ensure it's applied to grayscale images)\n",
    "    contrast_weight = [cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), cv2.CV_32F) for img in images]\n",
    "    contrast_weight = [np.abs(lap) for lap in contrast_weight]\n",
    "    contrast_weight = [np.repeat(lap[:, :, np.newaxis], 3, axis=2) for lap in contrast_weight]  # Convert to 3-channel\n",
    "\n",
    "    # Compute saliency weight (make sure this is in the same shape as the image)\n",
    "    saliency_weight = [np.std(img, axis=2) for img in images]\n",
    "    saliency_weight = [np.repeat(saliency[:, :, np.newaxis], 3, axis=2) for saliency in saliency_weight]  # Convert to 3-channel\n",
    "\n",
    "    # Compute exposure weight (ensure it's applied to RGB images)\n",
    "    exposure_weight = [np.exp(-((img - 0.5) ** 2) / (2 * 0.2 ** 2)) for img in images]\n",
    "    \n",
    "    # Combine weights\n",
    "    total_weight = [contrast_weight[i] + saliency_weight[i] + exposure_weight[i] + 1e-12 for i in range(len(images))]\n",
    "\n",
    "    weighted_images = [(w / sum(total_weight)) * img for w, img in zip(total_weight, images)]\n",
    "    \n",
    "    # Perform fusion\n",
    "    fused = sum(weighted_images)\n",
    "\n",
    "    # Convert back to uint8\n",
    "    fused = (fused * 255).astype(np.uint8)\n",
    "\n",
    "    return (\"Exposure Fusion\", fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of Exposure Fusion is the simplification of the inputs acquisition pipeline, as the computation of HDR images is not needed; the approach only relies on the simple quality measures used to compute the weights.\n",
    "\n",
    "An additional improvement which can be implemented is the application of a Gaussian filter to the weight maps; this results in a smoothing process, which helps reducing the seams disturbs caused by quick variations of the weights (this happens because the images we are combining contain different absolute intensities due to their different exposure times).\n",
    "\n",
    "The enhanced Exposure Fusion process is described by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_exposure_fusion(images, sigma=0.2, epsilon=1e-6, blur_kernel=(5,5)):\n",
    "    # Normalize images to [0, 1]\n",
    "    imgs = [img.astype(np.float32) / 255.0 for img in images]\n",
    "    \n",
    "    weight_maps = []\n",
    "    for img in imgs:\n",
    "        # Contrast weight: absolute Laplacian of the grayscale image\n",
    "        gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        contrast = np.abs(cv2.Laplacian(gray, cv2.CV_32F))\n",
    "        \n",
    "        # Saturation weight: standard deviation across color channels\n",
    "        saturation = np.std(img, axis=2)\n",
    "        \n",
    "        # Well-exposedness weight: Gaussian function per channel\n",
    "        well_exposedness = np.exp(-0.5 * ((img - 0.5) / sigma) ** 2)\n",
    "        # Combine well-exposedness across channels by taking the product\n",
    "        well_exposedness = np.prod(well_exposedness, axis=2)\n",
    "        \n",
    "        # Combine weights, adding a small constant to avoid zeros\n",
    "        weight = (contrast + epsilon) * (saturation + epsilon) * (well_exposedness + epsilon)\n",
    "        # Smooth the weight map to reduce abrupt transitions (black spots)\n",
    "        weight = cv2.GaussianBlur(weight, blur_kernel, 0)\n",
    "        weight_maps.append(weight)\n",
    "    \n",
    "    # Normalize the weight maps so that they sum to 1 at each pixel\n",
    "    weight_sum = np.sum(np.array(weight_maps), axis=0) + epsilon\n",
    "    normalized_weights = [w / weight_sum for w in weight_maps]\n",
    "    \n",
    "    # Fuse the images using the normalized weight maps\n",
    "    fused = np.zeros_like(imgs[0])\n",
    "    for img, w in zip(imgs, normalized_weights):\n",
    "        # Expand weight map to 3 channels for multiplication\n",
    "        w3 = np.repeat(w[:, :, np.newaxis], 3, axis=2)\n",
    "        fused += img * w3\n",
    "\n",
    "    # Convert back to uint8 in the range [0, 255]\n",
    "    fused_image = np.clip(fused * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return (\"Enhanced Exposure Fusion (Smoothed Weights)\", fused_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, smoothing the weight results in another issue, represented by undesirable halos around the edges and by information spilling across object boundaries. Instead, to address the problem, Mertens proposes a technique inspired by Burt and Andelson, in which images are blended by decomposing them into Laplacian Pyramids, before computing the weighted average for each level and reconstructing the resulting pyramid.\n",
    "\n",
    "The resulting procedure is implemented by OpenCV in the form of the `MergeMertens` class, whose function `process` weights pixels using contrast, saturation and well-exposedness and then combines the weighted images using Laplacian pyramids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mertens_fusion(images):\n",
    "    \"\"\"Fuse images using OpenCV's MergeMertens exposure fusion method.\"\"\"\n",
    "    utils_jupyter.imageCheck(images)\n",
    "    \n",
    "    merge_mertens = cv2.createMergeMertens()\n",
    "    fused = merge_mertens.process(images)\n",
    "    fused = np.clip(fused * 255, 0, 255).astype(np.uint8)\n",
    "    return ('Mertens Fusion', fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'Average Fusion': average_fusion,\n",
    "    'Laplacian Pyramid': laplacian_pyramid_fusion,\n",
    "    'Mertens Fusion': mertens_fusion,\n",
    "    'Exposure Fusion' : exposure_fusion,\n",
    "    'Exposure Compensation': exposure_compensation_fusion,\n",
    "    'Enhanced Exposure': enhanced_exposure_fusion,\n",
    "    'Domain Transform': domain_transform_fusion,\n",
    "    'Domain Transform - Homebrew': domain_transform_fusion,\n",
    "    'Wavelet Fusion': wavelet_fusion\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive MEF Method Explorer\n",
    "\n",
    "This widget showcases Multi-Exposure Fusion (MEF) techniques with real-time parameter tuning.\n",
    "Features tabbed interfaces for 8 fusion methods, including Laplacian Pyramid, Domain Transform, and Wavelet Fusion. Adjust sliders to optimize sigma/epsilon/levels and instantly visualize results with fused images and RGB histograms. Designed for rapid comparison of fusion outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_jupyter.showcase_methods_tab(images, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Transform Filter  \n",
    "A edge-preserving smoothing technique that:  \n",
    "- Treats the image as a 3D surface (x,y,intensity)  \n",
    "- Performs anisotropic diffusion along intensity gradients  \n",
    "- Controlled by two parameters:  \n",
    "  - σₛ (spatial): Controls geometric smoothness (60px in this implementation)  \n",
    "  - σ꜀ (color): Preserves edges with similar intensity (0.4 = ±102/255 intensity tolerance)  \n",
    "\n",
    "### Weight Map Calculation for Exposure Fusion  \n",
    "`calculate_weight_maps(img)` computes pixel-wise fusion weights using three quality metrics:  \n",
    "\n",
    "1. **Contrast** (∇²): Calculated via Laplacian operator on grayscale intensity, highlighting edges and textures where \n",
    "```math \n",
    "L(x,y) = |∂²I/∂x² + ∂²I/∂y²|\n",
    "``` \n",
    "\n",
    "2. **Saturation**: Measured as standard deviation across RGB channels, favoring vibrant colors  \n",
    " ```math\n",
    " σ = √(Σ(Iᵢ - μ)²/3)\n",
    " ```\n",
    "\n",
    "3. **Well-exposedness**: Gaussian probability that penalizes over/under-exposed pixels (μ=0.5 represents ideal mid-tone exposure)  \n",
    "```math\n",
    " exp(-(I-0.5)²/(2×0.2²)\n",
    " ```\n",
    "\n",
    "The combined weight map is computed as:  \n",
    "```math\n",
    "W(x,y) = (L(x,y)+ε) × (σ(x,y)+ε) × (E(x,y)+ε)\n",
    "```\n",
    "where ε=1e⁻⁶ ensures numerical stability during normalization.  \n",
    "\n",
    "The function returns:  \n",
    "1. Raw weight map  \n",
    "2. Homebrew DT-filtered version (iterative bilateral filtering approximation)  \n",
    "3. OpenCV's optimized DT-filter result  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_maps(img):\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    guidance = img.astype(np.float32) / 255.0\n",
    "\n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    contrast = np.abs(cv2.Laplacian(gray, cv2.CV_32F))\n",
    "\n",
    "    saturation = np.std(img, axis=2)\n",
    "\n",
    "    well_exposedness = np.exp(-0.5 * ((img - 0.5) / 0.2) ** 2)\n",
    "    well_exposedness = np.prod(well_exposedness, axis=2)\n",
    "\n",
    "    weight = (contrast + epsilon) * (saturation + epsilon) * (well_exposedness + epsilon)\n",
    "\n",
    "    # Apply Homebrew Domain Transform filter\n",
    "    smooth_weight_homebrew = imgTransformations.dt_filter_homebrew(guidance, weight, sigmaSpatial=60, sigmaColor=0.4, num_iterations=2)\n",
    "\n",
    "    # Apply OpenCV's dtFilter\n",
    "    smooth_weight_opencv = cv2.ximgproc.dtFilter(img, weight.astype(np.float32), sigmaSpatial=60, sigmaColor=0.4, mode=1)\n",
    "    \n",
    "    return weight, smooth_weight_homebrew, smooth_weight_opencv\n",
    "\n",
    "\n",
    "utils_jupyter.showcase_weight_maps_tab(images, calculate_weight_maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison between homebrew and opencv dt filter\n",
    "title, fused = methods[\"Domain Transform\"](images, homebrew_dt=False)\n",
    "title_hb, fused_hb = methods[\"Domain Transform\"](images, homebrew_dt=True)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(fused)\n",
    "plt.title(title)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(fused_hb)\n",
    "plt.title(title_hb)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Methods\n",
    "\n",
    "#### Visualize wavelet fusion\n",
    "\n",
    "Visualizes the **wavelet-based fusion** process on a **single color channel**.\n",
    "\n",
    "##### Features:\n",
    "- Extracts one color channel (default: Green)\n",
    "- Displays:\n",
    "  - The original input channel(s)\n",
    "  - Horizontal detail coefficients at each decomposition level\n",
    "  - Final fused image reconstructed from fused coefficients\n",
    "\n",
    "Fusion is performed by averaging approximation coefficients and applying a max-absolute rule for detail coefficients. Final image is reconstructed via inverse wavelet transform and displayed separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_wavelet_fusion(images, wavelet='db1', level=2, channel_index=1):\n",
    "    \"\"\"\n",
    "    Visualizes the steps of wavelet fusion for a single color channel.\n",
    "    Assumes all images are the same size and have 3 color channels (RGB).\n",
    "    \"\"\"\n",
    "    # Extract a single channel (e.g., the green channel) for visualization\n",
    "    channel_name = ['Blue', 'Green', 'Red'][channel_index]\n",
    "    imgs = [cv2.split(img)[channel_index].astype(np.float32) / 255.0 for img in images]\n",
    "\n",
    "    # Compute wavelet coefficients\n",
    "    coeffs_list = [pywt.wavedec2(img, wavelet=wavelet, level=level) for img in imgs]\n",
    "\n",
    "    # Set up grid: rows = images, columns = original + detail levels\n",
    "    fig, axes = plt.subplots(len(imgs), level + 1, figsize=(4 * (level + 1), 4 * len(imgs) + 4))\n",
    "    if len(imgs) == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    fig.suptitle(f\"Wavelet Fusion Steps for {channel_name} Channel (Single Channel Only)\", fontsize=16, y=0.95)\n",
    "\n",
    "    # Step 1: Show original channel\n",
    "    for i, img in enumerate(imgs):\n",
    "        axes[i, 0].imshow(img, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Image {i+1} - Original Channel')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "    # Step 2: Show decomposition details (just horizontal detail)\n",
    "    for i, coeffs in enumerate(coeffs_list):\n",
    "        for lvl in range(1, level + 1):\n",
    "            cH, cV, cD = coeffs[lvl]\n",
    "            axes[i, lvl].imshow(np.abs(cH), cmap='hot')\n",
    "            axes[i, lvl].set_title(f'Image {i+1} - Level {lvl} H Detail')\n",
    "            axes[i, lvl].axis('off')\n",
    "\n",
    "    # Step 3: Fuse coefficients\n",
    "    fused_coeffs = []\n",
    "\n",
    "    # Fuse approximation\n",
    "    fused_approx = np.mean([coeffs[0] for coeffs in coeffs_list], axis=0)\n",
    "    fused_coeffs.append(fused_approx)\n",
    "\n",
    "    # Fuse details\n",
    "    for lvl in range(1, level + 1):\n",
    "        fused_details = []\n",
    "        for i_type in range(3):  # Horizontal, vertical, diagonal\n",
    "            detail_coeffs = np.array([coeffs[lvl][i_type] for coeffs in coeffs_list])\n",
    "            fused_detail = np.choose(np.argmax(np.abs(detail_coeffs), axis=0), detail_coeffs)\n",
    "            fused_details.append(fused_detail)\n",
    "        fused_coeffs.append(tuple(fused_details))\n",
    "\n",
    "    # Step 4: Reconstruction\n",
    "    fused_img = pywt.waverec2(fused_coeffs, wavelet=wavelet)\n",
    "    h, w = imgs[0].shape\n",
    "    fused_img = fused_img[:h, :w]\n",
    "    fused_img = np.clip(fused_img, 0, 1)\n",
    "\n",
    "    # Step 5: Show fused image in new axis\n",
    "    fig_fused, ax_fused = plt.subplots(figsize=(5, 5))\n",
    "    ax_fused.imshow(fused_img, cmap='gray')\n",
    "    ax_fused.set_title(f'Fused Image - {channel_name} Channel')\n",
    "    ax_fused.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_wavelet_fusion(images, wavelet='db1', level=2, channel_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize laplacian pyramid fusion\n",
    "\n",
    "This function visualizes the **Laplacian Pyramid Fusion** process step by step. It shows:\n",
    "\n",
    "- The **Laplacian pyramids** of each input image\n",
    "- The **fused Laplacian pyramid**\n",
    "- Optionally, the **Gaussian pyramid** of the first image\n",
    "- The final **reconstructed fused image**\n",
    "\n",
    "Each image is first decomposed into a Laplacian pyramid, layers are fused by averaging, and the final image is reconstructed. Visualization is arranged in a grid, with a final display of the full fused result.\n",
    "\n",
    "##### Helper function: `normalize_for_display`\n",
    "Used to normalize each image layer to the `[0, 1]` range for visualization. This ensures that even negative or small values from Laplacian layers can be meaningfully displayed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(img):\n",
    "    \"\"\"Normalize an image layer to [0, 1] for visualization.\"\"\"\n",
    "    img_disp = img.copy()\n",
    "    if img_disp.ndim == 3:\n",
    "        for c in range(img_disp.shape[2]):\n",
    "            ch = img_disp[..., c]\n",
    "            img_disp[..., c] = cv2.normalize(ch, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    else:\n",
    "        img_disp = cv2.normalize(img_disp, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    return img_disp\n",
    "\n",
    "def visualize_laplacian_pyramid_fusion(images, levels=4, show_gaussian=True):\n",
    "    \"\"\"\n",
    "    Improved visualization of Laplacian Pyramid Fusion:\n",
    "    - Shows Gaussian + Laplacian pyramids\n",
    "    - Normalizes layers for display\n",
    "    - Displays final fused image\n",
    "    \"\"\"\n",
    "    is_color = images[0].ndim == 3 and images[0].shape[2] == 3\n",
    "    images = [img.astype(np.float32) for img in images]\n",
    "\n",
    "    # === Build Gaussian pyramids ===\n",
    "    gaussian_pyramids = []\n",
    "    for img in images:\n",
    "        gp = [img]\n",
    "        for _ in range(levels):\n",
    "            img = cv2.pyrDown(img)\n",
    "            gp.append(img)\n",
    "        gaussian_pyramids.append(gp)\n",
    "\n",
    "    # === Build Laplacian pyramids ===\n",
    "    laplacian_pyramids = []\n",
    "    for gp in gaussian_pyramids:\n",
    "        lp = []\n",
    "        for i in range(len(gp) - 1):\n",
    "            GE = cv2.pyrUp(gp[i + 1], dstsize=(gp[i].shape[1], gp[i].shape[0]))\n",
    "            L = gp[i] - GE\n",
    "            lp.append(L)\n",
    "        lp.append(gp[-1])  # Lowest level\n",
    "        laplacian_pyramids.append(lp)\n",
    "\n",
    "    # === Fuse Laplacian pyramids ===\n",
    "    fused_pyramid = []\n",
    "    for level in range(levels + 1):\n",
    "        stacked = np.array([lp[level] for lp in laplacian_pyramids])\n",
    "        fused_layer = np.mean(stacked, axis=0)\n",
    "        fused_pyramid.append(fused_layer)\n",
    "\n",
    "    # === Reconstruct final fused image ===\n",
    "    fused = fused_pyramid[-1]\n",
    "    for i in range(levels, 0, -1):\n",
    "        fused = cv2.pyrUp(fused, dstsize=(fused_pyramid[i-1].shape[1], fused_pyramid[i-1].shape[0]))\n",
    "        fused = fused + fused_pyramid[i-1]\n",
    "    fused = np.clip(fused, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # === Visualization ===\n",
    "    rows = len(images)\n",
    "    pyramid_cols = levels + 1\n",
    "    total_rows = rows + 1 + (1 if show_gaussian else 0)\n",
    "\n",
    "    fig, axes = plt.subplots(total_rows, pyramid_cols, figsize=(4 * pyramid_cols, 3 * total_rows))\n",
    "    fig.suptitle(f\"Laplacian Pyramid Fusion Visualization (levels={levels})\", fontsize=16, y=0.95)\n",
    "\n",
    "    # --- Show Laplacian pyramids of input images ---\n",
    "    for i, lp in enumerate(laplacian_pyramids):\n",
    "        for l in range(pyramid_cols):\n",
    "            layer_vis = normalize_for_display(lp[l])\n",
    "            if is_color:\n",
    "                layer_vis = cv2.cvtColor(layer_vis, cv2.COLOR_BGR2RGB)\n",
    "            axes[i, l].imshow(layer_vis)\n",
    "            axes[i, l].set_title(f\"Img {i+1} - Lap Level {l}\")\n",
    "            axes[i, l].axis('off')\n",
    "\n",
    "    # --- Fused Laplacian pyramid layers ---\n",
    "    for l in range(pyramid_cols):\n",
    "        layer_vis = normalize_for_display(fused_pyramid[l])\n",
    "        if is_color:\n",
    "            layer_vis = cv2.cvtColor(layer_vis, cv2.COLOR_BGR2RGB)\n",
    "        axes[rows, l].imshow(layer_vis)\n",
    "        axes[rows, l].set_title(f\"Fused - Lap Level {l}\")\n",
    "        axes[rows, l].axis('off')\n",
    "\n",
    "    # --- Optional: show Gaussian pyramid of first image ---\n",
    "    if show_gaussian:\n",
    "        gp0 = gaussian_pyramids[0]\n",
    "        for l in range(pyramid_cols):\n",
    "            g_layer = normalize_for_display(gp0[l])\n",
    "            if is_color:\n",
    "                g_layer = cv2.cvtColor(g_layer, cv2.COLOR_BGR2RGB)\n",
    "            axes[rows + 1, l].imshow(g_layer)\n",
    "            axes[rows + 1, l].set_title(f\"Gaussian Level {l}\")\n",
    "            axes[rows + 1, l].axis('off')\n",
    "\n",
    "    # --- Show final fused image in a separate figure ---\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Final fused output\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    fused_vis = cv2.cvtColor(fused, cv2.COLOR_BGR2RGB) if is_color else fused\n",
    "    plt.imshow(fused_vis)\n",
    "    plt.title(\"Final Fused Image (Reconstructed)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_laplacian_pyramid_fusion(images, levels=4, show_gaussian=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality Analysis Functions\n",
    "\n",
    "### 1. Entropy Calculation\n",
    "\n",
    "Measures the **information content** in an image using Shannon entropy. For each RGB channel:\n",
    "\n",
    "1. **Histogram Creation**:  \n",
    "   Counts pixel intensity occurrences across 256 bins (0-255 range).\n",
    "\n",
    "2. **Probability Distribution**:  \n",
    "   Converts counts to probabilities:  \n",
    "   ```math\n",
    "   p(i) = \\frac{\\text{count}(i)}{\\text{total pixels}}\n",
    "   ```\n",
    "\n",
    "3. **Entropy Calculation**:  \n",
    "   Computes average surprise per pixel:  \n",
    "   ```math\n",
    "   H = -\\sum_{i=0}^{255} p(i) \\cdot \\log_2 p(i)\n",
    "   ```  \n",
    "   (Zero probabilities are excluded to avoid mathematical errors)\n",
    "\n",
    "The final result averages entropy values across all three color channels.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Spatial Frequency Calculation\n",
    "Quantifies **edge activity** through gradient analysis. For each RGB channel:\n",
    "\n",
    "1. **Horizontal Variation (RF)**:  \n",
    "   Measures row-wise differences:  \n",
    "   ```math\n",
    "   RF = \\sqrt{\\frac{1}{MN} \\sum_{x=1}^{M-1} \\sum_{y=0}^{N-1} [I(x,y) - I(x-1,y)]^2}\n",
    "   ```\n",
    "\n",
    "2. **Vertical Variation (CF)**:  \n",
    "   Measures column-wise differences:  \n",
    "   ```math\n",
    "   CF = \\sqrt{\\frac{1}{MN} \\sum_{x=0}^{M-1} \\sum_{y=1}^{N-1} [I(x,y) - I(x,y-1)]^2}\n",
    "   ```\n",
    "\n",
    "3. **Combined Metric**:  \n",
    "   Merges directions using vector magnitude:  \n",
    "   ```math\n",
    "   SF = \\sqrt{RF^2 + CF^2}\n",
    "   ```\n",
    "\n",
    "The final output averages spatial frequency across all channels.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Characteristics Comparison\n",
    "\n",
    "| Aspect              | Entropy                          | Spatial Frequency               |\n",
    "|---------------------|----------------------------------|----------------------------------|\n",
    "| **What it measures** | Randomness in pixel values      | Intensity changes between pixels |\n",
    "| **High values mean** | Complex textures/variation      | Sharp edges/strong transitions   |\n",
    "| **Mathematical base**| Information theory              | Gradient energy analysis         |\n",
    "| **Typical range**   | 0-8 bits (8bpp images)          | 0-50 (image size dependent)      |\n",
    "| **Usage scenario**  | Assessing texture preservation  | Evaluating edge sharpness        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(image):\n",
    "    \"\"\"Calculates entropy for an RGB image by averaging entropy across channels.\"\"\"\n",
    "    entropy_values = []\n",
    "    for channel in range(image.shape[-1]):  # Loop through R, G, B channels\n",
    "        hist, _ = np.histogram(image[:, :, channel].ravel(), bins=256, range=(0, 256))\n",
    "        hist = hist.astype(np.float32) / hist.sum()\n",
    "        hist = hist[hist > 0]  # Remove zero probabilities\n",
    "        entropy_values.append(-np.sum(hist * np.log2(hist)))\n",
    "    return np.mean(entropy_values)  # Average entropy across channels\n",
    "\n",
    "def spatial_frequency(image):\n",
    "    \"\"\"Calculates spatial frequency for an RGB image.\"\"\"\n",
    "    sf_values = []\n",
    "    for channel in range(image.shape[-1]):  # Compute SF for each color channel\n",
    "        rows, cols = image.shape[:2]\n",
    "        row_freq = np.sqrt(np.sum(np.diff(image[:, :, channel], axis=0) ** 2) / (rows * cols))\n",
    "        col_freq = np.sqrt(np.sum(np.diff(image[:, :, channel], axis=1) ** 2) / (rows * cols))\n",
    "        sf_values.append(np.sqrt(row_freq**2 + col_freq**2))\n",
    "    return np.mean(sf_values)  # Average SF across channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Similarity Metrics: SSIM and PSNR\n",
    "\n",
    "### Overview\n",
    "\n",
    "The function `calculate_metrics_comparison` compares two images using two popular similarity metrics:\n",
    "- **SSIM (Structural Similarity Index Measure)**\n",
    "- **PSNR (Peak Signal-to-Noise Ratio)**\n",
    "\n",
    "These metrics help quantify how similar two images are, which is particularly useful in image processing tasks such as image compression, restoration, and quality assessment.\n",
    "\n",
    "### 1. SSIM (Structural Similarity Index Measure)\n",
    "\n",
    "#### Mathematical Explanation\n",
    "SSIM measures the perceptual similarity between two images by comparing their luminance, contrast, and structural information. Although the complete formula is complex, a simplified version is:\n",
    "\n",
    "```math\n",
    "SSIM(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1)(2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n",
    "```\n",
    "\n",
    "where:\n",
    "- $\\mu_x $ and $ \\mu_y $ are the mean intensities of images `x` and `y`.\n",
    "- $ \\sigma_x^2 $ and $ \\sigma_y^2 $ are the variances of `x` and `y`.\n",
    "- $ \\sigma_{xy} $ is the covariance between `x` and `y`.\n",
    "- $ C_1 $ and $ C_2 $ are constants to stabilize the division when denominators are close to zero.\n",
    "\n",
    "### 2. PSNR (Peak Signal-to-Noise Ratio)\n",
    "\n",
    "#### Mathematical Explanation\n",
    "PSNR measures the quality of a reconstructed image compared to its original version. It is calculated using the Mean Squared Error (MSE) between the images. The formula for PSNR is:\n",
    "\n",
    "```math\n",
    "PSNR = 20 \\cdot \\log_{10}(MAX_I) - 10 \\cdot \\log_{10}(MSE)\n",
    "```\n",
    "\n",
    "where:\n",
    "- $ MAX_I $ is the maximum possible pixel value (255 for 8-bit images).\n",
    "- **MSE** is the Mean Squared Error between the two images, defined as:\n",
    "\n",
    "```math\n",
    "MSE = \\frac{1}{M \\times N} \\sum_{x,y} \\left[ I_1(x, y) - I_2(x, y) \\right]^2\n",
    "```\n",
    "\n",
    "If the MSE is zero (meaning the images are identical), PSNR is set to infinity.\n",
    "\n",
    "### Process of the Algorithm\n",
    "\n",
    "1. **Input Validation and Preprocessing:**\n",
    "   - **Dimension Check:** The function first verifies whether the two images have the same shape. If not, it resizes the second image to match the dimensions of the first.\n",
    "   - **Grayscale Conversion:** Both images are converted from RGB to grayscale, as SSIM and PSNR are typically calculated on single-channel images.\n",
    "\n",
    "2. **SSIM Calculation:**\n",
    "   - The SSIM metric is computed using the grayscale images to assess the structural similarity between them.\n",
    "\n",
    "3. **PSNR Calculation:**\n",
    "   - **MSE Calculation:** The Mean Squared Error between the two grayscale images is computed.\n",
    "   - **PSNR Formula:** If the MSE is zero, PSNR is set to infinity. Otherwise, PSNR is calculated using the logarithmic formula based on the MSE.\n",
    "\n",
    "4. **Output:**\n",
    "   - The function returns two values: the SSIM value (`ssim_value`) and the PSNR value (`psnr_value`).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **SSIM** provides a measure of the perceptual similarity between images by taking into account luminance, contrast, and structural differences.\n",
    "- **PSNR** quantifies the error between images, with higher values indicating better image quality.\n",
    "- Together, these metrics are essential for comparing image quality and guiding improvements in image processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_comparison(img1, img2):\n",
    "    \"\"\"Calculate similarity metrics between two images\"\"\"\n",
    "    if img1.shape != img2.shape:\n",
    "        img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    # Convert to grayscale for SSIM/PSNR\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Calculate SSIM\n",
    "    ssim_value = ssim(gray1, gray2, data_range=255)\n",
    "    \n",
    "    # Calculate PSNR with error handling\n",
    "    mse = np.mean((gray1 - gray2) ** 2)\n",
    "    if mse == 0:\n",
    "        psnr_value = np.inf\n",
    "    else:\n",
    "        with np.errstate(divide='ignore'):\n",
    "            psnr_value = 20 * np.log10(255) - 10 * np.log10(mse)\n",
    "    \n",
    "    return ssim_value, psnr_value\n",
    "\n",
    "utils_jupyter.compare_methods_and_metrics(spatial_frequency, calculate_entropy, calculate_metrics_comparison, images, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Images with Heatmaps and RGB Histograms\n",
    "\n",
    "This snippet performs the following tasks:\n",
    "\n",
    "- **Compute Difference:**  \n",
    "  Calculates the absolute per-pixel difference between two images to highlight changes.\n",
    "\n",
    "- **Generate Heatmap:**  \n",
    "  Converts the difference image to grayscale and displays it as a heatmap using a \"hot\" color map to visualize intensity differences.\n",
    "\n",
    "- **Display Comparisons:**  \n",
    "  Uses a utility function to compare multiple images by showing their difference heatmaps alongside their RGB histograms, providing insights into color distribution and changes.\n",
    "\n",
    "This approach is useful for tasks like image quality assessment and change detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference(image1, image2):\n",
    "    \"\"\"Compute absolute difference between two images.\"\"\"\n",
    "    return cv2.absdiff(image1, image2)\n",
    "\n",
    "def show_heatmap(diff_image):\n",
    "    \"\"\"Display a heatmap of the differences.\"\"\"\n",
    "    diff_gray = cv2.cvtColor(diff_image, cv2.COLOR_RGB2GRAY)\n",
    "    plt.imshow(diff_gray, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Difference Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "utils_jupyter.compare_methods_display(images, compute_difference, show_heatmap, methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
